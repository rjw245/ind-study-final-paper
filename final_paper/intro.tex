\section{Introduction}

\paragraph{The Motivation for Specialization}
It used to be that as transistors grew smaller, chip power consumption would remain constant due to a principle known as Dennard scaling~\cite{dennardscaling}. Since 2006, Dennard scaling has begun to fail due to the fact that sub-micron sized transistors are more susceptible to leakage current and other non-ideal phenomena~\cite{dennard_end}. This means that placing more transistors on chip incurs greater power consumption, yet modern computers are unable to consume any more power and still be kept cool. For example, the power consumption of Intel CPUs plateaued back in 2006~\cite{hruska_2012}. To continue increasing the number of transistors on chip and stay within power budget, designers will have to selectively under-clock or turn off portions of the chip. As the number of transistors on chip grows, a smaller and smaller percentage of them will be usable at any given time. This \textit{utilization wall}, as it is called by Venkatesh et al.~\cite{Venkatesh:2010}, threatens to limit the extent to which computers can benefit from increasing transistor counts in the future.

Several solutions to the utilization wall have been proposed, including shrinking the number of transistors on chip, ``dimming'' regions of the chip, developing new, more efficient transistor technologies, and specializing groups of transistors. The first of these, shrinking transistor counts, clearly means the end of Moore's Law scaling. This has been called the ``most pessimistic'' solution to the problem~\cite{Taylor:2012}, and understandably, many in industry are hesitant to make such a big concession. ``Dimming'' technologies make the transistors on chip more energy-efficient, thereby increasing the number of transistors that can be used while staying within power budgets. However, these techniques often cause a decrease in performance. Dynamic voltage and frequency scaling (DVFS) is a common technique to reduce the power consumption of transistors when high performance is not required. Some work has been done to make DVFS even more effective for modern architectures~\cite{better_dvfs}. Other work targets energy efficiency by trying to make transistors more reusable. For example, DySER from the University of Wisconsin-Madison is an element of the processor pipeline that can be reconfigured to implement one of multiple functions~\cite{dyser}. Because DySER is reconfigurable, energy is only expended on the data path that is currently configured. This saves energy compared to architectures where idle data paths still consume power. The third approach---the development of new transistor tech---addresses the failure of Dennard scaling by abandoning CMOS altogether. Some of the work in this field includes tunnel field-effect transistors (TFETs)~\cite{tfets} which use quantum tunneling to implement a switch, and nano electro-mechanical switches (NEMS)~\cite{nems} which take advantage of the power efficiency of nano-switch technology. Lastly, specialization addresses power limitations by spreading computation across an array of specialized blocks of transistors, called accelerators. Rather than have a monolithic processor do the majority of computation and offload a small portion to specialized hardware (as is the norm today), the vision for specialization in the future has computation bouncing between many specialized regions of the chip~\cite{Taylor:2012}. Because there are more transistors on chip than can be used at any one time, area can be ``spent'' on accelerators specialized for certain computations. These accelerators use fewer transistors than a general-purpose processor to implement specific functionality. Thus, on a fully specialized system, the computer will use a minimal number of transistors at any given time, thereby maintaining performance gains despite the utilization wall.

Accelerators are already being adopted in industry. The International Technology Roadmap for Semiconductors (ITRS) released a report in 2007 which predicted that chips will contain almost 1500 accelerators by the year 2022~\cite{itrs:2007}. Today, accelerators can be found in common devices like the iPhone~\cite{Shao:2015} and the PlayStation 3~\cite{Kistler:2006}. Many have looked to specialization as a promising answer to the utilization wall, but for accelerators to solve the problem entirely, they will need to become pervasive and numerous in future architectures. Integrating accelerators in large numbers and moving away from large, general-purpose processors requires significant work in the development of new architectures.

\paragraph{The Challenges for Accelerator-Rich Architectures}
The shift toward specialized computation brings about new challenges for architectures in areas such as on-chip communication, memory structure, resource sharing and contention between cores, and others. As computation becomes less centralized, it will be important for accelerators to communicate with one another without the assistance of a central controller (which would become a bottleneck as the number of accelerators grows). This is also called streaming. To stream data from one accelerator to another, the accelerators must all agree on a common interface. Architectures have been developed which allow for such an interface, such as the Accelerator Store which uses FIFOs to send data between accelerators~\cite{Lyons:2010}. They do this to avoid using a DMA engine, which they believe would artificially constrain the interfaces of accelerators in the system. Conversely, another architecture, AXR-CMP~\cite{Cong:2012}, uses a DMA to interface the accelerators. These two architectures are at odds in terms of how they implement streaming.

\paragraph{The Value of Accelerator-Rich Applications}
Mention the RoboBee~\cite{robobee}.


\paragraph{Requirements for the Testbed Architecture}
Talk about the requirements of the application (streaming, processor level of access) and the needs of researchers (Dave in specific, who needs the memory interconnect to be interchangeable with his own custom design. Hence the "MODULAR INTERCONNECT" block. But I guess you talk about that specifically in the technical documentation).

